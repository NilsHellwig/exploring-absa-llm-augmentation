{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d22bba-6bd8-4cf5-a82d-8902e533973c",
   "metadata": {},
   "source": [
    "# Notebook: Analyse Language Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dcfb1-6eab-4730-bda2-ca3a90d80987",
   "metadata": {},
   "source": [
    "This notebook is used to analyse the language diversity of all conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa234d8-c34b-4aa5-a156-a7faf6555811",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e001435c-ddc2-4522-9c2f-5cb7ec941aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b526110f-23cd-481d-a46e-c237e2b19721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c272eaaa-e283-4a2d-89d0-65cfa446bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nils_hellwig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc5537f-e0b7-4d17-9d98-7c8a3c9f8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c26256-a86f-4545-94a0-da98d5cf9d01",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d62364c6-b0f8-42c7-b6ff-99da24cecb8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_REAL = 0\n",
    "N_SYNTH = 500\n",
    "LABELS_FIXED = True\n",
    "MODEL_NAME = \"Llama13B\" # \"Llama70B\", \"GPT-3\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b678461-d604-4c91-86a0-b4c843a52f49",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829019d1-c2fc-4b6b-bcfc-89ff1a568b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"Llama70B\", \"GPT-3\"]\n",
    "SAMPLE_COUNT = [500, 1000, 2000]\n",
    "SAMPLING_STRATEGY = [True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba7e45-2617-4526-8bd4-f479848f628f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eeb129b-44e0-455f-ba71-83ac329f2abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_tokens(texts):\n",
    "    token_counts = [] \n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        token_counts.append(len(tokens))\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2254fe-973c-4513-a4ea-7040202381a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_lemmas(texts):\n",
    "    unique_lemmas = set()\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            unique_lemmas.add(token.lemma_)\n",
    "    return len(unique_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "935d6b48-6643-47b7-a634-bd454375fe53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_and_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS and token.text not in string.punctuation]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def count_top_n_lemmas(texts, n):\n",
    "    lemma_counts = {}\n",
    "    for text in texts:\n",
    "        cleaned_text = remove_stopwords_and_punctuation(text)\n",
    "        doc = nlp(cleaned_text)\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            if lemma in lemma_counts:\n",
    "                lemma_counts[lemma] += 1\n",
    "            else:\n",
    "                lemma_counts[lemma] = 1\n",
    "    \n",
    "    sorted_lemmas = sorted(lemma_counts, key=lambda lemma: lemma_counts[lemma], reverse=True)\n",
    "    top_n_lemmas = sorted_lemmas[:n]\n",
    "    \n",
    "    return ', '.join(top_n_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a47b0424-7629-4b12-a941-d3008fb1132d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_language_statistic(n_synth, n_real, labels_fixed, model_name):\n",
    "    total_texts = []\n",
    "    unique_lemma_counts = []\n",
    "    texts_token_counts = []\n",
    "    n_aspects_total = []\n",
    "    n_implicit_aspects_total = []\n",
    "    n_explicit_aspects_total = []\n",
    "    total_llm_invalid_xml_schema = 0\n",
    "    total_llm_invalid_xml_tags = 0\n",
    "    total_llm_aspect_polarity_in_text_but_not_in_label = 0\n",
    "    total_llm_more_than_one_sentences = 0\n",
    "    total_llm_no_german_language = 0\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        print(i)\n",
    "        examples_in_split = []\n",
    "    \n",
    "        # Load Real Split\n",
    "        real_path = f\"../07 train classifier/real/split_{i}.json\"\n",
    "        with open(real_path, 'r') as file:\n",
    "            real_data = json.load(file)[:n_real]\n",
    "            examples_in_split = real_data\n",
    "    \n",
    "        # Load Synth Split\n",
    "        if n_synth > 0:\n",
    "            fake_path = f\"../07 train classifier/synth/{model_name}/{'fixed' if labels_fixed else 'random'}/split_{i}.json\"\n",
    "            with open(fake_path, 'r') as file:\n",
    "                fake_data = json.load(file)[:n_synth]\n",
    "                examples_in_split = fake_data\n",
    "        \n",
    "        texts = [example[\"text\"] for example in examples_in_split]\n",
    "    \n",
    "    \n",
    "        # Calculate n tokens in texts\n",
    "        texts_token_count = count_tokens(texts)\n",
    "        for count in texts_token_count:\n",
    "            texts_token_counts.append(count)\n",
    "    \n",
    "        # Calcuate unique lemmas in text\n",
    "        unique_lemma_count = count_unique_lemmas(texts)\n",
    "        unique_lemma_counts.append(unique_lemma_count)\n",
    "    \n",
    "        # Calculate number of aspects (implicit+explicit)\n",
    "        n_aspects = len([tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"]])\n",
    "        n_aspects_total.append(n_aspects)\n",
    "    \n",
    "    \n",
    "        # Calculate number of implicit aspects\n",
    "        n_implicit_aspects = len([tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"] if tag[\"text\"] is None])\n",
    "        n_implicit_aspects_total.append(n_implicit_aspects)\n",
    "    \n",
    "        # Calculate number of unique aspect terms\n",
    "        explicit_aspects = [tag[\"text\"] for example in examples_in_split for tag in example[\"tags\"] if tag[\"text\"] is not None]\n",
    "        n_unique_aspect_terms = len(set(explicit_aspects))\n",
    "        n_explicit_aspects_total.append(n_unique_aspect_terms)\n",
    "        \n",
    "        print(examples_in_split[0][\"llm_invalid_xml_schema\"])\n",
    "        # Add to total count\n",
    "        for example in examples_in_split:\n",
    "            total_llm_invalid_xml_schema += example[\"llm_invalid_xml_schema\"]\n",
    "            total_llm_invalid_xml_tags += example[\"llm_invalid_xml_tags\"]\n",
    "            total_llm_aspect_polarity_in_text_but_not_in_label += example[\"llm_aspect_polarity_in_text_but_not_in_label\"]\n",
    "            total_llm_more_than_one_sentences += example[\"llm_more_than_one_sentences\"]\n",
    "            total_llm_no_german_language += example[\"llm_no_german_language\"]\n",
    "    \n",
    "    \n",
    "        # Add to total text collection\n",
    "        total_texts.extend(texts)\n",
    "        \n",
    "    top_n_lemmas = count_top_n_lemmas(total_texts, 5)\n",
    "    unique_lemmas_avg = np.mean(unique_lemma_counts)\n",
    "    texts_token_counts_avg = np.mean(texts_token_counts)\n",
    "    texts_token_counts_sd = np.std(texts_token_counts)\n",
    "    n_aspects_avg = np.mean(n_aspects_total)\n",
    "    n_implicit_aspects_avg = np.mean(n_implicit_aspects_total) / (np.mean(n_implicit_aspects_total) + np.mean(n_explicit_aspects_total))\n",
    "    n_explicit_aspects_avg = np.mean(n_explicit_aspects_total) / (np.mean(n_implicit_aspects_total) + np.mean(n_explicit_aspects_total))\n",
    "    \n",
    "    statistic = {\n",
    "      \"condition\": f\"{n_synth} fake\" if n_synth > 0 else (f\"{n_real} fake\" if n_real > 0 else \"unknown condition\"),\n",
    "      \"llm\": model_name,\n",
    "      \"few-shot examples\": \"fixed\" if labels_fixed else \"random\",\n",
    "      \"top_n_lemmas\": top_n_lemmas,\n",
    "      \"unique_lemmas_avg\": unique_lemmas_avg,\n",
    "      \"texts_token_counts_avg\": texts_token_counts_avg,\n",
    "      \"texts_token_counts_sd\": texts_token_counts_sd,\n",
    "      \"n_aspects_avg\": n_aspects_avg,\n",
    "      \"n_implicit_aspects_avg\": n_implicit_aspects_avg,\n",
    "      \"n_explicit_aspects_avg\": n_explicit_aspects_avg,\n",
    "      \"total_llm_invalid_xml_schema\": total_llm_invalid_xml_schema, # Summe aller invaliden retries Ã¼ber alle 5 folds hinweg\n",
    "      \"total_llm_invalid_xml_tags\": total_llm_invalid_xml_tags,\n",
    "      \"total_llm_aspect_polarity_in_text_but_not_in_label\": total_llm_aspect_polarity_in_text_but_not_in_label,\n",
    "      \"total_llm_more_than_one_sentences\": total_llm_more_than_one_sentences,\n",
    "      \"total_llm_no_german_language\": total_llm_no_german_language,\n",
    "      \"total_llm_retries\": total_llm_invalid_xml_schema + total_llm_invalid_xml_tags + total_llm_aspect_polarity_in_text_but_not_in_label + total_llm_more_than_one_sentences + total_llm_no_german_language\n",
    "    }\n",
    "\n",
    "    return statistic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d6e56e-3bfd-4218-b38b-048d224d6f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "4\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'condition': '5 fake',\n",
       "  'llm': 'Llama13B',\n",
       "  'few-shot examples': 'fixed',\n",
       "  'top_n_lemmas': 'Service, Preis, essen, LOC, Speise',\n",
       "  'unique_lemmas_avg': 42.4,\n",
       "  'texts_token_counts_avg': 13.2,\n",
       "  'texts_token_counts_sd': 7.919595949289332,\n",
       "  'n_aspects_avg': 12.4,\n",
       "  'n_implicit_aspects_avg': 0.2833333333333333,\n",
       "  'n_explicit_aspects_avg': 0.7166666666666667,\n",
       "  'total_llm_invalid_xml_schema': 0,\n",
       "  'total_llm_invalid_xml_tags': 0,\n",
       "  'total_llm_aspect_polarity_in_text_but_not_in_label': 3,\n",
       "  'total_llm_more_than_one_sentences': 11,\n",
       "  'total_llm_no_german_language': 1,\n",
       "  'total_llm_retries': 15}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = []\n",
    "statistics.append(get_language_statistic(5, 0, True, \"Llama13B\"))\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4444e027-4b67-4373-b9b5-38bea8574ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synth_count in SAMPLE_COUNT:\n",
    "    for sampling_strategy in SAMPLING_STRATEGY:\n",
    "        for model in MODELS:\n",
    "            pass\n",
    "            #statistics.append(get_language_statistic(synth_count, 0, sampling_strategy, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02791222-a855-4263-93a3-b2a0b8da0cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'llm_invalid_xml_schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m real_count \u001b[38;5;129;01min\u001b[39;00m SAMPLE_COUNT:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# model and sampling strategy are irrelevant. only real ones are considered anyway\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(real_count)\n\u001b[0;32m----> 4\u001b[0m     statistics\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_language_statistic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODELS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[10], line 57\u001b[0m, in \u001b[0;36mget_language_statistic\u001b[0;34m(n_synth, n_real, labels_fixed, model_name)\u001b[0m\n\u001b[1;32m     54\u001b[0m n_unique_aspect_terms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(explicit_aspects))\n\u001b[1;32m     55\u001b[0m n_explicit_aspects_total\u001b[38;5;241m.\u001b[39mappend(n_unique_aspect_terms)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mexamples_in_split\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm_invalid_xml_schema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Add to total count\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples_in_split:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'llm_invalid_xml_schema'"
     ]
    }
   ],
   "source": [
    "for real_count in SAMPLE_COUNT:\n",
    "    # model and sampling strategy are irrelevant. only real ones are considered anyway\n",
    "    print(real_count)\n",
    "    statistics.append(get_language_statistic(0, real_count, False, MODELS[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887b77f-c2e3-446c-b296-f4340b4fb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba0132-6de0-432d-adaf-ae7f08bd3b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"language_statistics.json\", 'w') as json_file:\n",
    "    json.dump(statistics, json_file, indent=4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
