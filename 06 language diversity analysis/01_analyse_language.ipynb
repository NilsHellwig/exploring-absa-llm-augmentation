{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Analyse Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nils_hellwig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORIES = [\"GENERAL-IMPRESSION\",\n",
    "                     \"FOOD\", \"SERVICE\", \"AMBIENCE\", \"PRICE\"]\n",
    "LLMS = [\"GPT-3\", \"Llama70B\"]\n",
    "FS_CONDITIONS = [\"fixed\", \"random\"]\n",
    "PROMPTING_ENCODING = {\"fixed\": \"25 fixed examples\",\n",
    "                      \"random\": \"25 random examples\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(texts):\n",
    "    token_counts = [] \n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        token_counts.append(len(tokens))\n",
    "    return token_counts\n",
    "\n",
    "def count_unique_lemmas(texts):\n",
    "    unique_lemmas = set()\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            unique_lemmas.add(token.lemma_)\n",
    "    return len(unique_lemmas)\n",
    "\n",
    "def remove_stopwords_and_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS and token.text not in string.punctuation and token.text.isalpha()]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def count_top_n_lemmas(texts, n):\n",
    "    lemma_counts = {}\n",
    "    for text in texts:\n",
    "        cleaned_text = remove_stopwords_and_punctuation(text)\n",
    "        doc = nlp(cleaned_text)\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            if lemma in lemma_counts:\n",
    "                lemma_counts[lemma] += 1\n",
    "            else:\n",
    "                lemma_counts[lemma] = 1\n",
    "    \n",
    "    sorted_lemmas = sorted(lemma_counts, key=lambda lemma: lemma_counts[lemma], reverse=True)\n",
    "    top_n_lemmas = sorted_lemmas[:n]\n",
    "    \n",
    "    return ', '.join(top_n_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"synth\": {}, \"real\": []}\n",
    "\n",
    "# Load Synth\n",
    "for llm in LLMS:\n",
    "    dataset[\"synth\"][llm] = {}\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        dataset[\"synth\"][llm][prompting] = []\n",
    "        for split in range(5):\n",
    "            with open(f\"../07 train models/synth/{llm}/{prompting}/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "                split_data = json.load(json_file)\n",
    "            for example in split_data:\n",
    "                example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "            dataset[\"synth\"][llm][prompting].append(split_data)\n",
    "\n",
    "# Load Real\n",
    "for split in range(5):\n",
    "    with open(f\"../07 train models/real/split_{split}.json\", 'r', encoding='utf-8') as json_file:\n",
    "        split_data = json.load(json_file)\n",
    "    for example in split_data:\n",
    "        example[\"tokenized_text\"] = nlp(example[\"text\"])\n",
    "    dataset[\"real\"].append(split_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Avg Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "GPT-3 fixed 9.68\n",
      "GPT-3 fixed [9.24, 9.72, 9.1, 10.32, 10.01]\n",
      "-----\n",
      "GPT-3 random 9.04\n",
      "GPT-3 random [8.97, 9.11, 8.84, 8.87, 9.4]\n",
      "-----\n",
      "Llama70B fixed 10.31\n",
      "Llama70B fixed [9.53, 10.93, 9.98, 10.31, 10.78]\n",
      "-----\n",
      "Llama70B random 10.16\n",
      "Llama70B random [10.02, 10.22, 9.85, 10.18, 10.55]\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        print(\"-----\")\n",
    "        print(llm, prompting, round(np.mean(count_tokens(\n",
    "            [example[\"text\"] for split_data in dataset[\"synth\"][llm][prompting] for example in split_data])), 2))\n",
    "        print(llm, prompting, [round(np.mean(count_tokens(\n",
    "            [example[\"text\"] for example in dataset[\"synth\"][llm][prompting][split_id]])), 2) for split_id in range(0, 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real 13.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Real\", round(np.mean(count_tokens([example[\"text\"] for split_examples in dataset[\"real\"] for example in split_examples])), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_level_levenshtein_distance(docs):\n",
    "    tokenized_texts = [[token.text for token in doc] for doc in docs]\n",
    "\n",
    "    total_distance = 0\n",
    "    pair_count = 0\n",
    "\n",
    "    for i in range(len(tokenized_texts)):\n",
    "        for j in range(i + 1, len(tokenized_texts)):\n",
    "            tokens1 = tokenized_texts[i]\n",
    "            tokens2 = tokenized_texts[j]\n",
    "\n",
    "            if len(tokens1) >= len(tokens2):\n",
    "                max_tokens = len(tokens1)\n",
    "            else:\n",
    "                max_tokens = len(tokens2)\n",
    "\n",
    "            distance = Levenshtein.distance(tokens1, tokens2)\n",
    "            total_distance += distance\n",
    "            pair_count += 1\n",
    "\n",
    "    average_distance = total_distance / pair_count if pair_count > 0 else 0\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 fixed 0 9.43767509266028\n",
      "GPT-3 fixed 1 10.021982994113348\n",
      "GPT-3 fixed 2 9.306075179869955\n",
      "GPT-3 fixed 3 10.383843638182448\n",
      "GPT-3 fixed 4 9.958813948418042\n",
      "GPT-3 random 0 8.851007338225484\n",
      "GPT-3 random 1 8.96883878140983\n",
      "GPT-3 random 2 8.621337780742717\n",
      "GPT-3 random 3 8.47982477207027\n",
      "GPT-3 random 4 9.342684456304204\n",
      "Llama70B fixed 0 10.288666076718863\n",
      "Llama70B fixed 1 11.788395983224962\n",
      "Llama70B fixed 2 10.640098495633104\n",
      "Llama70B fixed 3 11.24434868480115\n",
      "Llama70B fixed 4 11.343140317802316\n",
      "Llama70B random 0 10.895543695797198\n",
      "Llama70B random 1 11.07879653102068\n",
      "Llama70B random 2 10.53895574827663\n",
      "Llama70B random 3 10.937361796753391\n",
      "Llama70B random 4 11.44266044029353\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        for split_idx in range(5):\n",
    "            texts_in_split = [example[\"tokenized_text\"] for example in dataset[\"synth\"][llm][prompting][split_idx]]\n",
    "            print(llm, prompting, split_idx, average_word_level_levenshtein_distance(texts_in_split))\n",
    "\n",
    "            # for ac in ASPECT_CATEGORIES:\n",
    "            #     tags_in_split = [nlp(tag[\"text\"]) for example in dataset[\"synth\"][llm][prompting][split_idx] for tag in example[\"tags\"] if tag[\"type\"] == \"label-explicit\" and tag[\"label\"] == ac]\n",
    "            #     print(llm, prompting, split_idx, ac, average_word_level_cosine_similarity(tags_in_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent terms\n",
    "\n",
    "Ähnlich wie bei den realen Daten Aspekte, die das Aspekt selber benennen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(456.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3\n",
      "\n",
      " 25 fixed examples & GENERAL-IMPRESSION & 161 & 22.4 & \\textit{Restaurant} (71.4), \\textit{Eindruck} (35.0), \\textit{Gesamteindruck} (12.4), \\textit{allgemeine Impression} (5.4), \\textit{Atmosphäre} (4.6) \\\\\n",
      "\n",
      " & FOOD  & 479 & 23.4 & \\textit{Essen} (405.2), \\textit{Speisen} (12.2), \\textit{Dessert} (11.8), \\textit{Gericht} (7.4), \\textit{Pizza} (5.8) \\\\\n",
      "\n",
      " & SERVICE  & 509 & 8.8 & \\textit{Service} (414.4), \\textit{Personal} (62.4), \\textit{Bedienung} (14.2), \\textit{Servicepersonal} (11.0), \\textit{Kellner} (3.2) \\\\\n",
      "\n",
      " & AMBIENCE  & 482 & 17.6 & \\textit{Ambiente} (278.8), \\textit{Atmosphäre} (101.8), \\textit{Restaurant} (54.0), \\textit{Musik} (14.6), \\textit{Einrichtung} (12.4) \\\\\n",
      "\n",
      " & PRICE  & 403 & 10.6 & \\textit{Preise} (300.4), \\textit{Preis} (32.6), \\textit{Preis-Leistungs-Verhältnis} (32.2), \\textit{Preis-Leistungsverhältnis} (18.2), \\textit{Preisniveau} (7.8) \\\\ \\hline\n",
      "\n",
      " 25 random examples & GENERAL-IMPRESSION & 78 & 13.0 & \\textit{Restaurant} (50.6), \\textit{Eindruck} (10.2), \\textit{Gesamteindruck} (3.0), \\textit{Service} (2.0), \\textit{allgemeine Impression} (1.8) \\\\\n",
      "\n",
      " & FOOD  & 276 & 18.6 & \\textit{Essen} (229.0), \\textit{Speisen} (14.0), \\textit{Pizza} (6.8), \\textit{Gericht} (3.2), \\textit{Steak} (2.8) \\\\\n",
      "\n",
      " & SERVICE  & 309 & 7.4 & \\textit{Service} (256.4), \\textit{Personal} (28.8), \\textit{Bedienung} (16.8), \\textit{Servicepersonal} (3.0), \\textit{Kellner} (1.0) \\\\\n",
      "\n",
      " & AMBIENCE  & 315 & 11.6 & \\textit{Ambiente} (163.8), \\textit{Atmosphäre} (80.4), \\textit{Restaurant} (44.2), \\textit{Einrichtung} (13.2), \\textit{Musik} (5.2) \\\\\n",
      "\n",
      " & PRICE  & 203 & 12.2 & \\textit{Preise} (125.4), \\textit{Preis-Leistungs-Verhältnis} (24.0), \\textit{Preis} (21.6), \\textit{Preis-Leistungsverhältnis} (15.2), \\textit{Preisniveau} (5.8) \\\\ \\hline\n",
      "Llama70B\n",
      "\n",
      " 25 fixed examples & GENERAL-IMPRESSION & 352 & 95.8 & \\textit{Lokal} (40.8), \\textit{Essen} (40.8), \\textit{Restaurant} (39.0), \\textit{Ambiente} (27.4), \\textit{Atmosphäre} (26.4) \\\\\n",
      "\n",
      " & FOOD  & 479 & 85.6 & \\textit{Essen} (261.0), \\textit{Pizza} (33.0), \\textit{Speisen} (27.4), \\textit{Küche} (12.8), \\textit{Gerichte} (10.2) \\\\\n",
      "\n",
      " & SERVICE  & 508 & 38.2 & \\textit{Service} (141.8), \\textit{Personal} (133.6), \\textit{Bedienung} (127.0), \\textit{Servicepersonal} (19.4), \\textit{Kellner} (19.0) \\\\\n",
      "\n",
      " & AMBIENCE  & 485 & 64.4 & \\textit{Ambiente} (222.0), \\textit{Atmosphäre} (134.4), \\textit{Lokal} (10.0), \\textit{Lage} (9.0), \\textit{Essen} (6.0) \\\\\n",
      "\n",
      " & PRICE  & 437 & 48.8 & \\textit{Preise} (150.8), \\textit{Preis} (80.8), \\textit{Preis-Leistungsverhältnis} (49.2), \\textit{Essen} (27.8), \\textit{Preis/Leistung} (20.8) \\\\ \\hline\n",
      "\n",
      " 25 random examples & GENERAL-IMPRESSION & 216 & 75.8 & \\textit{Restaurant} (30.8), \\textit{Essen} (30.2), \\textit{Atmosphäre} (14.2), \\textit{Ambiente} (12.4), \\textit{LOC} (9.2) \\\\\n",
      "\n",
      " & FOOD  & 334 & 76.2 & \\textit{Essen} (162.2), \\textit{Speisen} (30.0), \\textit{Pizza} (21.8), \\textit{Küche} (11.0), \\textit{Gerichte} (7.6) \\\\\n",
      "\n",
      " & SERVICE  & 343 & 41.0 & \\textit{Service} (117.6), \\textit{Bedienung} (79.8), \\textit{Personal} (57.6), \\textit{Servicepersonal} (13.8), \\textit{Essen} (11.8) \\\\\n",
      "\n",
      " & AMBIENCE  & 347 & 62.8 & \\textit{Ambiente} (153.0), \\textit{Atmosphäre} (87.6), \\textit{Terrasse} (4.8), \\textit{Umgebung} (4.6), \\textit{Lage} (4.4) \\\\\n",
      "\n",
      " & PRICE  & 310 & 47.6 & \\textit{Preise} (99.0), \\textit{Preis} (77.4), \\textit{Preis-Leistungsverhältnis} (24.4), \\textit{Essen} (21.6), \\textit{Preis/Leistungsverhältnis} (10.6) \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    print(llm)\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        for ac_idx, aspect_category in enumerate(ASPECT_CATEGORIES):\n",
    "            aspect_terms_all_splits = []\n",
    "            n_aspects_in_splits = []\n",
    "            n_aspect_terms_in_splits = []\n",
    "            n_unique_aspects_in_split = []\n",
    "\n",
    "            for split_idx in range(5):\n",
    "                aspects_in_split = [tag for example in dataset[\"synth\"][llm][prompting][split_idx]\n",
    "                                    for tag in example[\"tags\"] if tag[\"label\"] == aspect_category]\n",
    "                aspect_terms_in_split = [\n",
    "                    tag[\"text\"] for tag in aspects_in_split if tag[\"type\"] == \"label-explicit\"]\n",
    "                aspect_terms_all_splits += aspect_terms_in_split\n",
    "                n_aspects_in_splits.append(len(aspects_in_split))\n",
    "                n_aspect_terms_in_splits.append(len(aspect_terms_in_split))\n",
    "                n_unique_aspects_in_split.append(\n",
    "                    len(list(set(aspect_terms_in_split))))\n",
    "\n",
    "            aspect_term_counts = Counter(aspect_terms_all_splits)\n",
    "            most_common_aspect_terms = aspect_term_counts.most_common(5)\n",
    "\n",
    "            term_list = [\n",
    "                f\"\\\\textit{{{term}}} ({round(count/5,2)})\" for term, count in most_common_aspect_terms]\n",
    "            term_string = \", \".join(term_list)\n",
    "\n",
    "            if ac_idx == 0:\n",
    "                print(\n",
    "                    f\"\\n {PROMPTING_ENCODING[prompting]} & {aspect_category} & {round(np.mean(n_aspect_terms_in_splits))} & {round(np.mean(n_unique_aspects_in_split), 2)} & {term_string} \\\\\\\\\")\n",
    "            elif ac_idx == 4:\n",
    "                print(\n",
    "                    f\"\\n & {aspect_category}  & {round(np.mean(n_aspect_terms_in_splits))} & {round(np.mean(n_unique_aspects_in_split), 2)} & {term_string} \\\\\\\\ \\\\hline\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"\\n & {aspect_category}  & {round(np.mean(n_aspect_terms_in_splits))} & {round(np.mean(n_unique_aspects_in_split), 2)} & {term_string} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prozentualer Anteil an Aspektbegriffen je Aspekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 fixed total: 0.6528686946476704 splits: [0.5901639344262295, 0.5418544752092723, 0.7245912151330555, 0.623593699774992, 0.7827751196172249] 0.08846750333202695 0.007826499145802199\n",
      "GPT-3 random total: 0.5666410601113885 splits: [0.5739503816793893, 0.5501460564751705, 0.5521085797382452, 0.6105577689243028, 0.5485636114911081] 0.02362742458662041 0.0005582551925964346\n",
      "Llama70B fixed total: 0.7271791572853007 splits: [0.754650416933932, 0.6447751536719508, 0.7412407585985213, 0.7285300739787713, 0.7660462130937099] 0.04302372886341045 0.0018510412453122578\n",
      "Llama70B random total: 0.7422660664687291 splits: [0.7538022813688213, 0.7445255474452555, 0.7443318861553304, 0.7433981066268062, 0.726158038147139] 0.008978311412013579 8.061007581109327e-05\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        aspect_terms_total = []\n",
    "        n_aspects_total = 0\n",
    "        ratio_splits = []\n",
    "        for split_idx in range(5):\n",
    "            aspect_terms_split = []\n",
    "            n_aspects_split = 0\n",
    "            for example in dataset[\"synth\"][llm][prompting][split_idx]:\n",
    "                aspect_terms = [tag[\"text\"] for tag in example[\"tags\"] if tag[\"type\"] == \"label-explicit\"]\n",
    "                n_aspects_example = len([tag for tag in example[\"tags\"]])\n",
    "\n",
    "                aspect_terms_split += aspect_terms\n",
    "                n_aspects_split += n_aspects_example\n",
    "\n",
    "        \n",
    "            aspect_terms_total += aspect_terms_split\n",
    "            n_aspects_total += n_aspects_split\n",
    "            ratio_splits.append(len(aspect_terms_split) / n_aspects_split)\n",
    "                \n",
    "        print(llm, prompting, \"total:\", len(aspect_terms_total) / n_aspects_total, \"splits:\", ratio_splits, np.std(ratio_splits), np.var(ratio_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erster Token im Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: für normale daten berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prozentsatz der Artikel (GPT-3, fixed): 91.47341772151898%\n",
      "Prozentsatz der Artikel (GPT-3, random): 91.66666666666666%\n",
      "Prozentsatz der Artikel (Llama70B, fixed): 57.72151898734177%\n",
      "Prozentsatz der Artikel (Llama70B, random): 57.333333333333336%\n"
     ]
    }
   ],
   "source": [
    "for llm in LLMS:\n",
    "    for prompting in FS_CONDITIONS:\n",
    "        first_pos = [example[\"tokenized_text\"][0].pos_ for split_idx in range(5) for example in dataset[\"synth\"][llm][prompting][split_idx]]\n",
    "        pos_counts = Counter(first_pos)\n",
    "        article_percentage = (pos_counts[\"DET\"] / len(first_pos)) * 100\n",
    "        print(f\"Prozentsatz der Artikel ({llm}, {prompting}): {article_percentage}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
